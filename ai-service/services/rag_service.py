"""
RAG Service
Retrieval Augmented Generation - T√¨m ki·∫øm context v√† generate c√¢u tr·∫£ l·ªùi

NOTE: This service is being deprecated in favor of the new Orchestrator.
Keeping for backward compatibility.
"""
from typing import List, Optional, Dict, Any
import time
import asyncio

from core.config import settings
from services.orchestrator import orchestrator


class RAGService:
    """Service x·ª≠ l√Ω RAG pipeline - now using Orchestrator"""
    
    def __init__(self):
        """Initialize with new Orchestrator"""
        self.orchestrator = orchestrator
        print("‚úÖ RAGService initialized with Multi-Model Orchestrator")
    
    async def query_with_orchestrator(
        self,
        question: str,
        user_id: str,
        document_ids: Optional[List[str]] = None,
        top_k: int = None,
        score_threshold: float = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> Dict[str, Any]:
        """
        Query using new Multi-Model Orchestrator
        
        This is the NEW recommended method that supports:
        - Intent classification
        - Multi-model routing
        - Direct chat (no documents needed)
        - Smart fallback
        
        Args:
            question: User's question
            user_id: User ID
            document_ids: Optional document IDs
            top_k: Number of contexts
            score_threshold: Score threshold
            temperature: LLM temperature
            max_tokens: Max tokens
        
        Returns:
            Dict with answer, contexts, intent, model, etc.
        """
        try:
            # Use defaults if not provided
            top_k = top_k or settings.RAG_TOP_K
            score_threshold = score_threshold or settings.RAG_SCORE_THRESHOLD
            
            # Call orchestrator
            result = await self.orchestrator.process_query(
                question=question,
                user_id=user_id,
                document_ids=document_ids,
                top_k=top_k,
                score_threshold=score_threshold,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return result
        
        except Exception as e:
            raise Exception(f"Orchestrator query error: {e}")
    
    def classify_query_type(self, question: str) -> str:
        """
        Ph√¢n lo·∫°i c√¢u h·ªèi ƒë·ªÉ √°p d·ª•ng strategy ph√π h·ª£p
        
        Returns:
            'factual': H·ªèi th√¥ng tin c·ª• th·ªÉ t·ª´ t√†i li·ªáu
            'creative': Y√™u c·∫ßu s√°ng t·∫°o, t·∫°o c√¢u h·ªèi m·ªõi, brainstorm
            'analytical': Ph√¢n t√≠ch, so s√°nh, t·ªïng h·ª£p
        """
        question_lower = question.lower()
        
        # Creative indicators
        creative_keywords = [
            "ƒë∆∞a ra th√™m c√¢u h·ªèi", "t·∫°o c√¢u h·ªèi", "g·ª£i √Ω c√¢u h·ªèi",
            "c√¢u h·ªèi kh√°c", "c√¢u h·ªèi t∆∞∆°ng t·ª±", "brainstorm",
            "√Ω t∆∞·ªüng", "s√°ng t·∫°o", "th√™m c√¢u h·ªèi"
        ]
        if any(kw in question_lower for kw in creative_keywords):
            return "creative"
        
        # Analytical indicators  
        analytical_keywords = [
            "so s√°nh", "kh√°c nhau", "gi·ªëng nhau", "ph√¢n t√≠ch",
            "t·∫°i sao", "l√†m th·∫ø n√†o", "t·ªïng h·ª£p", "ƒë√°nh gi√°"
        ]
        if any(kw in question_lower for kw in analytical_keywords):
            return "analytical"
        
        # Default: factual
        return "factual"
    
    def search_relevant_contexts(
        self,
        query: str,
        user_id: str,
        document_ids: Optional[List[str]] = None,
        top_k: int = 5,
        score_threshold: float = 0.5,
        enable_fallback: bool = True
    ) -> List[Dict[str, Any]]:
        """
        T√¨m ki·∫øm contexts li√™n quan t·ª´ Qdrant v·ªõi fallback strategy
        
        Args:
            query: C√¢u h·ªèi/query
            user_id: User ID ƒë·ªÉ filter
            document_ids: Optional list of document IDs ƒë·ªÉ filter
            top_k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£
            score_threshold: Ng∆∞·ª°ng similarity score
            enable_fallback: Cho ph√©p fallback n·∫øu kh√¥ng ƒë·ªß k·∫øt qu·∫£
        
        Returns:
            List[Dict]: Danh s√°ch contexts v·ªõi metadata
        """
        try:
            # Generate query embedding
            query_vector = embedding_service.embed_query(query)
            
            # Build filter
            filter_conditions = [
                FieldCondition(
                    key="user_id",
                    match=MatchValue(value=user_id)
                )
            ]
            
            # Add document_ids filter if provided
            if document_ids:
                filter_conditions.append(
                    FieldCondition(
                        key="document_id",
                        match=MatchAny(any=document_ids)
                    )
                )
            
            query_filter = Filter(must=filter_conditions) if filter_conditions else None
            
            # Search in Qdrant
            search_results = qdrant_manager.client.search(
                collection_name=qdrant_manager.collection_name,
                query_vector=query_vector,
                query_filter=query_filter,
                limit=top_k,
                score_threshold=score_threshold
            )
            
            # FALLBACK STRATEGY: N·∫øu kh√¥ng ƒë·ªß k·∫øt qu·∫£, gi·∫£m threshold
            if enable_fallback and len(search_results) < max(2, top_k // 2):
                min_threshold = settings.RAG_MIN_SCORE_THRESHOLD
                if score_threshold > min_threshold:
                    print(f"‚ö†Ô∏è Fallback: Gi·∫£m threshold t·ª´ {score_threshold} xu·ªëng {min_threshold}")
                    search_results = qdrant_manager.client.search(
                        collection_name=qdrant_manager.collection_name,
                        query_vector=query_vector,
                        query_filter=query_filter,
                        limit=top_k,
                        score_threshold=min_threshold
                    )
            
            # Format results
            contexts = []
            for result in search_results:
                context = {
                    "chunk_id": result.id,
                    "score": result.score,
                    "chunk_text": result.payload.get("chunk_text", ""),
                    "chunk_index": result.payload.get("chunk_index", 0),
                    "document_id": result.payload.get("document_id", ""),
                    "file_name": result.payload.get("file_name", ""),
                    "title": result.payload.get("title", ""),
                }
                contexts.append(context)
            
            return contexts
        
        except Exception as e:
            raise Exception(f"Context search error: {e}")
    
    def build_rag_prompt(
        self,
        question: str,
        contexts: List[Dict[str, Any]]
    ) -> str:
        """
        Build prompt cho RAG v·ªõi contexts
        
        Args:
            question: C√¢u h·ªèi c·ªßa user
            contexts: Danh s√°ch contexts t·ª´ vector search
        
        Returns:
            str: Prompt ƒë·∫ßy ƒë·ªß
        """
        # Build context string
        context_parts = []
        for idx, ctx in enumerate(contexts, 1):
            context_parts.append(
                f"[T√ÄI LI·ªÜU {idx}] - {ctx['file_name']}\n{ctx['chunk_text']}\n"
            )
        
        context_str = "\n".join(context_parts)
        
        # Build full prompt
        prompt = f"""B·∫°n l√† tr·ª£ l√Ω h·ªçc t·∫≠p th√¥ng minh, gi√∫p sinh vi√™n tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu h·ªçc t·∫≠p.

NGUY√äN T·∫ÆC:
- Tr·∫£ l·ªùi d·ª±a CH√çNH X√ÅC v√†o n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p
- N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong t√†i li·ªáu, h√£y n√≥i r√µ "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu"
- Tr√≠ch d·∫´n ngu·ªìn khi tr·∫£ l·ªùi (v√≠ d·ª•: "Theo t√†i li·ªáu X...")
- Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu cho sinh vi√™n
- N·∫øu c√¢u h·ªèi kh√¥ng r√µ r√†ng, h√£y y√™u c·∫ßu l√†m r√µ

T√ÄI LI·ªÜU THAM KH·∫¢O:
{context_str}

C√ÇU H·ªéI: {question}

TR·∫¢ L·ªúI:"""
        
        return prompt
    
    def generate_answer(
        self,
        question: str,
        contexts: List[Dict[str, Any]],
        query_type: str = "factual",
        temperature: float = None,
        max_tokens: int = None
    ) -> tuple[str, int]:
        """
        Generate c√¢u tr·∫£ l·ªùi t·ª´ LLM using Chat API v·ªõi prompt ph√π h·ª£p theo query type
        
        Args:
            question: C√¢u h·ªèi g·ªëc
            contexts: Danh s√°ch contexts t·ª´ RAG
            query_type: Lo·∫°i c√¢u h·ªèi (factual/creative/analytical)
            temperature: Temperature (0-1)
            max_tokens: Max tokens
        
        Returns:
            tuple: (answer, tokens_used)
        """
        try:
            temperature = temperature or settings.LLM_TEMPERATURE
            max_tokens = max_tokens or settings.LLM_MAX_TOKENS
            
            # Build context section
            context_section = "T√ÄI LI·ªÜU THAM KH·∫¢O:\n"
            for idx, ctx in enumerate(contexts, 1):
                title = ctx.get("title", ctx.get("file_name", "Document"))
                score = ctx.get("score", 0)
                context_section += f"\n[T√ÄI LI·ªÜU {idx}] - {title} (ƒë·ªô li√™n quan: {score:.2f})\n{ctx['chunk_text']}\n"
            
            # Build system prompt based on query type
            if query_type == "creative":
                system_message = """B·∫°n l√† tr·ª£ l√Ω h·ªçc t·∫≠p s√°ng t·∫°o, gi√∫p sinh vi√™n m·ªü r·ªông ki·∫øn th·ª©c.

NHI·ªÜM V·ª§:
- D·ª±a v√†o n·ªôi dung t√†i li·ªáu ƒë·ªÉ hi·ªÉu ch·ªß ƒë·ªÅ v√† c√°c kh√°i ni·ªám
- S√°ng t·∫°o c√¢u h·ªèi m·ªõi, c√¢u h·ªèi suy lu·∫≠n, c√¢u h·ªèi m·ªü r·ªông
- C√¢u h·ªèi ph·∫£i li√™n quan ƒë·∫øn ki·∫øn th·ª©c trong t√†i li·ªáu nh∆∞ng c√≥ th·ªÉ ƒëi s√¢u h∆°n
- ƒê∆∞a ra c√¢u h·ªèi ·ªü nhi·ªÅu m·ª©c ƒë·ªô: d·ªÖ, trung b√¨nh, kh√≥
- Gi·∫£i th√≠ch ng·∫Øn g·ªçn t·∫°i sao c√¢u h·ªèi ƒë√≥ quan tr·ªçng

ƒê·ªäNH D·∫†NG:
1. **C√¢u h·ªèi**: [c√¢u h·ªèi]
   - **M·ª©c ƒë·ªô**: [d·ªÖ/trung b√¨nh/kh√≥]
   - **L√Ω do**: [t·∫°i sao c√¢u h·ªèi n√†y quan tr·ªçng]

"""
            elif query_type == "analytical":
                system_message = """B·∫°n l√† tr·ª£ l√Ω ph√¢n t√≠ch th√¥ng minh, gi√∫p sinh vi√™n hi·ªÉu s√¢u v·ªÅ ki·∫øn th·ª©c.

NHI·ªÜM V·ª§:
- Ph√¢n t√≠ch, so s√°nh, ƒë√°nh gi√° c√°c kh√°i ni·ªám trong t√†i li·ªáu
- T√¨m ra m·ªëi li√™n h·ªá, ƒëi·ªÉm gi·ªëng/kh√°c, ∆∞u/nh∆∞·ª£c ƒëi·ªÉm
- Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• c·ª• th·ªÉ v√† d·ªÖ hi·ªÉu
- C√≥ th·ªÉ s·ª≠ d·ª•ng ki·∫øn th·ª©c chung ƒë·ªÉ l√†m r√µ, nh∆∞ng ph·∫£i d·ª±a tr√™n t√†i li·ªáu

"""
            else:  # factual
                system_message = """B·∫°n l√† tr·ª£ l√Ω h·ªçc t·∫≠p ch√≠nh x√°c, gi√∫p sinh vi√™n t√¨m th√¥ng tin t·ª´ t√†i li·ªáu.

NHI·ªÜM V·ª§:
- Tr·∫£ l·ªùi d·ª±a CH√çNH X√ÅC v√†o n·ªôi dung t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p
- Tr√≠ch d·∫´n ngu·ªìn khi tr·∫£ l·ªùi (v√≠ d·ª•: "Theo t√†i li·ªáu X...")
- N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin, n√≥i r√µ "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong t√†i li·ªáu"
- Gi·∫£i th√≠ch r√µ r√†ng, d·ªÖ hi·ªÉu

"""
            
            # Build full message
            full_message = f"""{system_message}
{context_section}

C√ÇU H·ªéI: {question}

TR·∫¢ L·ªúI:"""
            
            # Adjust temperature for creative queries
            if query_type == "creative":
                temperature = min(1.0, temperature + 0.2)  # TƒÉng creativity
            
            # Call Chat API
            response = self.cohere_client.chat(
                model=self.llm_model,
                message=full_message,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            answer = response.text.strip()
            
            # Get token usage if available
            tokens_used = 0
            if hasattr(response, 'meta') and response.meta and hasattr(response.meta, 'tokens'):
                tokens_info = response.meta.tokens
                if hasattr(tokens_info, 'input_tokens') and hasattr(tokens_info, 'output_tokens'):
                    tokens_used = tokens_info.input_tokens + tokens_info.output_tokens
                else:
                    tokens_used = len(full_message.split()) + len(answer.split())
            else:
                # Estimate if not available
                tokens_used = len(full_message.split()) + len(answer.split())
            
            return answer, tokens_used
        
        except Exception as e:
            raise Exception(f"LLM generation error: {e}")
    
    def query(
        self,
        question: str,
        user_id: str,
        document_ids: Optional[List[str]] = None,
        top_k: int = None,
        score_threshold: float = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> Dict[str, Any]:
        """
        RAG query pipeline ho√†n ch·ªânh v·ªõi intelligent query processing
        
        Args:
            question: C√¢u h·ªèi
            user_id: User ID
            document_ids: Optional document IDs filter
            top_k: S·ªë contexts
            score_threshold: Score threshold
            temperature: LLM temperature
            max_tokens: Max tokens
        
        Returns:
            Dict v·ªõi answer, contexts, metadata
        """
        start_time = time.time()
        
        try:
            # Classify query type
            query_type = self.classify_query_type(question)
            print(f"üîç Query type detected: {query_type}")
            
            # Use defaults if not provided
            top_k = top_k or settings.RAG_TOP_K
            score_threshold = score_threshold or settings.RAG_SCORE_THRESHOLD
            
            # 1. Search relevant contexts with fallback
            contexts = self.search_relevant_contexts(
                query=question,
                user_id=user_id,
                document_ids=document_ids,
                top_k=top_k,
                score_threshold=score_threshold,
                enable_fallback=settings.RAG_ENABLE_FALLBACK
            )
            
            if not contexts:
                return {
                    "answer": "T√¥i kh√¥ng t√¨m th·∫•y t√†i li·ªáu ph√π h·ª£p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y. Vui l√≤ng upload th√™m t√†i li·ªáu ho·∫∑c th·ª≠ c√¢u h·ªèi kh√°c.",
                    "contexts": [],
                    "model": self.llm_model,
                    "tokens_used": 0,
                    "processing_time": time.time() - start_time,
                    "query_type": query_type
                }
            
            # Log contexts found
            scores_str = ", ".join([f"{c['score']:.2f}" for c in contexts[:3]])
            print(f"üìö Found {len(contexts)} contexts (scores: {scores_str})")
            
            # 2. Generate answer with appropriate prompt based on query type
            answer, tokens_used = self.generate_answer(
                question=question,
                contexts=contexts,
                query_type=query_type,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            processing_time = time.time() - start_time
            
            return {
                "answer": answer,
                "contexts": contexts,
                "model": self.llm_model,
                "tokens_used": tokens_used,
                "processing_time": processing_time,
                "query_type": query_type
            }
        
        except Exception as e:
            raise Exception(f"RAG query error: {e}")
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        user_id: str,
        document_ids: Optional[List[str]] = None,
        temperature: float = None,
        max_tokens: int = None
    ) -> Dict[str, Any]:
        """
        Chat v·ªõi context t·ª´ documents
        
        Args:
            messages: Chat history [{"role": "user", "content": "..."}]
            user_id: User ID
            document_ids: Optional document IDs
            temperature: Temperature
            max_tokens: Max tokens
        
        Returns:
            Dict v·ªõi response v√† metadata
        """
        try:
            # Get last user message
            last_message = messages[-1]["content"]
            
            # Search contexts based on last message
            contexts = self.search_relevant_contexts(
                query=last_message,
                user_id=user_id,
                document_ids=document_ids,
                top_k=3,  # Fewer contexts for chat
                score_threshold=0.75
            )
            
            # Build chat prompt with context
            if contexts:
                context_str = "\n\n".join([
                    f"[{ctx['file_name']}]: {ctx['chunk_text']}"
                    for ctx in contexts
                ])
                
                system_message = f"""B·∫°n l√† tr·ª£ l√Ω h·ªçc t·∫≠p. D·ª±a v√†o t√†i li·ªáu sau ƒë·ªÉ tr·∫£ l·ªùi:

{context_str}

Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c d·ª±a tr√™n t√†i li·ªáu."""
            else:
                system_message = "B·∫°n l√† tr·ª£ l√Ω h·ªçc t·∫≠p th√¥ng minh."
            
            # Build chat messages
            chat_messages = [{"role": "system", "content": system_message}]
            chat_messages.extend(messages)
            
            # Call Cohere chat API
            response = self.cohere_client.chat(
                message=last_message,
                chat_history=[
                    {"role": msg["role"], "message": msg["content"]}
                    for msg in messages[:-1]
                ] if len(messages) > 1 else None,
                preamble=system_message,
                model=self.llm_model,
                temperature=temperature or settings.LLM_TEMPERATURE,
                max_tokens=max_tokens or settings.LLM_MAX_TOKENS
            )
            
            return {
                "message": response.text,
                "contexts": contexts if contexts else None,
                "model": self.llm_model,
                "tokens_used": None  # Cohere chat doesn't return token count
            }
        
        except Exception as e:
            raise Exception(f"Chat error: {e}")


# Global instance
rag_service = RAGService()
