"""
AI Orchestrator - Multi-Model Query Processing
Main orchestrator that routes queries to appropriate handlers based on intent
"""
from typing import List, Dict, Optional, Any
import time

from core.config import settings
from core.model_manager import model_manager
from services.intent_classifier import intent_classifier
from services.embedding_service import embedding_service
from core.qdrant import qdrant_manager
from qdrant_client.models import Filter, FieldCondition, MatchValue, MatchAny


class AIOrchestrator:
    """
    Main orchestrator for AI queries
    Routes to appropriate handler based on intent classification
    """
    
    def __init__(self):
        """Initialize orchestrator"""
        self.model_manager = model_manager
        self.intent_classifier = intent_classifier
    
    async def process_query(
        self,
        question: str,
        user_id: str,
        document_ids: Optional[List[str]] = None,
        top_k: int = 5,
        score_threshold: float = 0.5,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        session_history: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Main entry point - process query with intent classification
        
        Args:
            question: User's question
            user_id: User ID for filtering
            document_ids: Optional document IDs
            top_k: Number of contexts to retrieve
            score_threshold: Similarity threshold
            temperature: LLM temperature
            max_tokens: Max tokens
            session_history: Chat history
        
        Returns:
            Dict with answer, contexts, intent, model, etc.
        """
        start_time = time.time()
        
        try:
            # 1. Classify intent
            has_docs = bool(document_ids and len(document_ids) > 0)
            intent = self.intent_classifier.classify(
                question=question,
                has_documents=has_docs,
                document_count=len(document_ids) if document_ids else 0
            )
            
            print(f"ðŸŽ¯ Intent: {intent} | Has docs: {has_docs} | Question: {question[:50]}...")
            
            # 2. Route to appropriate handler
            if intent == "direct_chat" or intent == "code_help":
                result = await self.handle_direct_chat(
                    question=question,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    session_history=session_history
                )
            
            elif intent == "rag_query":
                result = await self.handle_rag_query(
                    question=question,
                    user_id=user_id,
                    document_ids=document_ids,
                    top_k=top_k,
                    score_threshold=score_threshold,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
            
            elif intent == "summarization":
                result = await self.handle_summarization(
                    user_id=user_id,
                    document_ids=document_ids,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
            
            elif intent == "question_generation":
                result = await self.handle_question_generation(
                    question=question,
                    user_id=user_id,
                    document_ids=document_ids,
                    top_k=top_k,
                    temperature=temperature
                )
            
            elif intent == "homework_solver":
                result = await self.handle_homework(
                    question=question,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
            
            else:
                # Fallback to direct chat
                result = await self.handle_direct_chat(question, temperature, max_tokens)
            
            # 3. Add metadata
            result["intent"] = intent
            result["processing_time"] = time.time() - start_time
            result["user_id"] = user_id
            
            print(f"âœ… Completed in {result['processing_time']:.2f}s")
            
            return result
        
        except Exception as e:
            print(f"âŒ Orchestrator error: {e}")
            raise Exception(f"Query processing failed: {e}")
    
    async def handle_direct_chat(
        self,
        question: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        session_history: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Direct chat without RAG - for general questions, homework, coding
        Uses: Gemini Flash (fast, FREE)
        """
        try:
            # Select model
            provider_name, model_object = self.model_manager.get_model(
                task_type="direct_chat",
                complexity="low"
            )
            
            # System instruction
            system_instruction = """Báº¡n lÃ  trá»£ lÃ½ há»c táº­p thÃ´ng minh, giÃºp sinh viÃªn há»c táº­p hiá»‡u quáº£.

NHIá»†M Vá»¤:
- Tráº£ lá»i cÃ¢u há»i rÃµ rÃ ng, dá»… hiá»ƒu
- Giáº£i thÃ­ch chi tiáº¿t, cÃ³ vÃ­ dá»¥ minh há»a
- Vá»›i bÃ i táº­p: hÆ°á»›ng dáº«n tá»«ng bÆ°á»›c, giáº£i thÃ­ch logic
- Vá»›i code: viáº¿t code Ä‘Ãºng chuáº©n, comment Ä‘áº§y Ä‘á»§
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t

PHONG CÃCH:
- ThÃ¢n thiá»‡n, dá»… tiáº¿p cáº­n
- Khuyáº¿n khÃ­ch tÆ° duy pháº£n biá»‡n
- ÄÆ°a ra tips há»c táº­p"""
            
            # Generate answer
            answer = self.model_manager.generate_text(
                provider_name=provider_name,
                model_identifier=model_object,
                prompt=question,
                system_instruction=system_instruction,
                temperature=temperature or 0.7,
                max_tokens=max_tokens or 2048
            )
            
            return {
                "answer": answer,
                "contexts": [],
                "model": provider_name,
                "tokens_used": self._estimate_tokens(question, answer)
            }
        
        except Exception as e:
            # Fallback to Groq if Gemini fails
            print(f"âš ï¸ Primary model failed, trying fallback: {e}")
            return await self._fallback_chat(question, temperature, max_tokens)
    
    async def handle_rag_query(
        self,
        question: str,
        user_id: str,
        document_ids: Optional[List[str]],
        top_k: int = 5,
        score_threshold: float = 0.5,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        RAG query with document context
        Uses: Gemini Flash for simple, Gemini Pro for complex
        """
        try:
            # 1. Retrieve contexts from Qdrant
            contexts = await self._retrieve_contexts(
                query=question,
                user_id=user_id,
                document_ids=document_ids,
                top_k=top_k,
                score_threshold=score_threshold
            )
            
            if not contexts:
                return {
                    "answer": "TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin phÃ¹ há»£p trong tÃ i liá»‡u cá»§a báº¡n. Vui lÃ²ng thá»­ cÃ¢u há»i khÃ¡c hoáº·c upload thÃªm tÃ i liá»‡u.",
                    "contexts": [],
                    "model": "none",
                    "tokens_used": 0
                }
            
            # Log contexts found
            scores_preview = ", ".join([f"{c['score']:.2f}" for c in contexts[:3]])
            print(f"ðŸ“š Retrieved {len(contexts)} contexts (scores: {scores_preview})")
            
            # 2. Determine complexity and select model
            is_complex = self.intent_classifier.is_complex_query(question)
            provider_name, model_object = self.model_manager.get_model(
                task_type="rag_query",
                complexity="high" if is_complex else "low"
            )
            
            # 3. Build RAG prompt
            context_str = "\n\n".join([
                f"[TÃ i liá»‡u {i+1} - {ctx['file_name']}]\n{ctx['chunk_text']}"
                for i, ctx in enumerate(contexts)
            ])
            
            # System instruction
            system_instruction = """Báº¡n lÃ  trá»£ lÃ½ há»c táº­p thÃ´ng minh. Tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p.

NGUYÃŠN Táº®C:
- Tráº£ lá»i dá»±a CHÃNH XÃC vÃ o ná»™i dung tÃ i liá»‡u
- TrÃ­ch dáº«n nguá»“n khi tráº£ lá»i (vÃ­ dá»¥: "Theo TÃ i liá»‡u 1...")
- Náº¿u khÃ´ng tÃ¬m tháº¥y thÃ´ng tin, nÃ³i rÃµ "ThÃ´ng tin nÃ y khÃ´ng cÃ³ trong tÃ i liá»‡u"
- Giáº£i thÃ­ch rÃµ rÃ ng, dá»… hiá»ƒu
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t"""
            
            # Build user prompt
            user_prompt = f"""TÃ€I LIá»†U:
{context_str}

CÃ‚U Há»ŽI: {question}

TRáº¢ Lá»œI:"""
            
            # 4. Generate answer with fallback
            answer, used_model = self._generate_with_fallback(
                provider_name=provider_name,
                model_identifier=model_object,
                prompt=user_prompt,
                system_instruction=system_instruction,
                temperature=temperature or settings.LLM_TEMPERATURE,
                max_tokens=max_tokens or 2048
            )
            
            return {
                "answer": answer,
                "contexts": contexts,
                "model": used_model,
                "tokens_used": self._estimate_tokens(context_str + question, answer)
            }
        
        except Exception as e:
            print(f"âŒ RAG query error: {e}")
            raise Exception(f"RAG processing failed: {e}")
    
    async def handle_summarization(
        self,
        user_id: str,
        document_ids: Optional[List[str]],
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Summarize full document(s)
        Uses: Gemini Pro (2M context, good for long documents)
        """
        try:
            if not document_ids or len(document_ids) == 0:
                return {
                    "answer": "Vui lÃ²ng chá»n tÃ i liá»‡u cáº§n tÃ³m táº¯t.",
                    "contexts": [],
                    "model": "none",
                    "tokens_used": 0
                }
            
            # Retrieve ALL chunks from document (not just top-k)
            contexts = await self._retrieve_full_document(
                user_id=user_id,
                document_ids=document_ids
            )
            
            if not contexts:
                return {
                    "answer": "KhÃ´ng tÃ¬m tháº¥y ná»™i dung tÃ i liá»‡u.",
                    "contexts": [],
                    "model": "none",
                    "tokens_used": 0
                }
            
            # Use Gemini Pro for long context
            provider_name, model_object = self.model_manager.get_model(
                task_type="summarization",
                complexity="high"
            )
            
            # Build full document text
            full_text = "\n\n".join([
                f"[Pháº§n {i+1}]\n{ctx['chunk_text']}"
                for i, ctx in enumerate(contexts)
            ])
            
            # System instruction
            system_instruction = """Báº¡n lÃ  trá»£ lÃ½ tÃ³m táº¯t chuyÃªn nghiá»‡p.

NHIá»†M Vá»¤:
TÃ³m táº¯t ná»™i dung chÃ­nh cá»§a tÃ i liá»‡u theo cáº¥u trÃºc:

1. Tá»”NG QUAN (2-3 cÃ¢u)
2. CÃC ÄIá»‚M CHÃNH (bullet points)
3. Káº¾T LUáº¬N (1-2 cÃ¢u)

YÃªu cáº§u:
- Ngáº¯n gá»n, sÃºc tÃ­ch
- Náº¯m báº¯t Ã½ chÃ­nh
- Dá»… hiá»ƒu, rÃµ rÃ ng
- Báº±ng tiáº¿ng Viá»‡t"""
            
            # Build user prompt
            user_prompt = f"""TÃ€I LIá»†U:
{full_text}

TÃ“M Táº®T:"""
            
            # Generate summary with fallback
            answer, used_model = self._generate_with_fallback(
                provider_name=provider_name,
                model_identifier=model_object,
                prompt=user_prompt,
                system_instruction=system_instruction,
                temperature=temperature or 0.5,
                max_tokens=max_tokens or 2048
            )
            
            return {
                "answer": answer,
                "contexts": contexts[:10],  # Return sample contexts
                "model": used_model,
                "tokens_used": self._estimate_tokens(full_text, answer)
            }
        
        except Exception as e:
            print(f"âŒ Summarization error: {e}")
            raise Exception(f"Summarization failed: {e}")
    
    async def handle_question_generation(
        self,
        question: str,
        user_id: str,
        document_ids: Optional[List[str]],
        top_k: int = 10,
        temperature: Optional[float] = None
    ) -> Dict[str, Any]:
        """
        Generate questions from document content
        Uses: Gemini Pro (creative mode)
        """
        try:
            # Retrieve contexts
            contexts = await self._retrieve_contexts(
                query=question,
                user_id=user_id,
                document_ids=document_ids,
                top_k=top_k,
                score_threshold=0.3  # Lower threshold to get more content
            )
            
            if not contexts:
                return {
                    "answer": "KhÃ´ng tÃ¬m tháº¥y ná»™i dung Ä‘á»ƒ táº¡o cÃ¢u há»i. Vui lÃ²ng upload tÃ i liá»‡u.",
                    "contexts": [],
                    "model": "none",
                    "tokens_used": 0
                }
            
            # Use Gemini Pro for creativity
            provider_name, model_object = self.model_manager.get_model(
                task_type="question_generation",
                complexity="high"
            )
            
            # Build context
            context_str = "\n\n".join([ctx['chunk_text'] for ctx in contexts])
            
            # System instruction
            system_instruction = """Báº¡n lÃ  chuyÃªn gia táº¡o cÃ¢u há»i há»c táº­p.

NHIá»†M Vá»¤:
Dá»±a vÃ o kiáº¿n thá»©c trong tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p, táº¡o cÃ¡c cÃ¢u há»i Ä‘á»ƒ giÃºp sinh viÃªn há»c táº­p vÃ  Ã´n táº­p hiá»‡u quáº£.

YÃŠU Cáº¦U:
- Táº¡o Ã­t nháº¥t 5-10 cÃ¢u há»i
- PhÃ¢n loáº¡i theo má»©c Ä‘á»™: Dá»„ / TRUNG BÃŒNH / KHÃ“
- CÃ¢u há»i pháº£i liÃªn quan trá»±c tiáº¿p Ä‘áº¿n ná»™i dung tÃ i liá»‡u
- Äa dáº¡ng: tráº¯c nghiá»‡m, tá»± luáº­n, phÃ¢n tÃ­ch, so sÃ¡nh
- Má»—i cÃ¢u há»i kÃ¨m giáº£i thÃ­ch táº¡i sao nÃ³ quan trá»ng

Äá»ŠNH Dáº NG:
**CÃ¢u 1** (Dá»…): [CÃ¢u há»i]
- *LÃ½ do*: [Táº¡i sao cÃ¢u há»i nÃ y quan trá»ng]

**CÃ¢u 2** (Trung bÃ¬nh): [CÃ¢u há»i]
- *LÃ½ do*: [Táº¡i sao cÃ¢u há»i nÃ y quan trá»ng]"""
            
            # Build user prompt
            user_prompt = f"""KIáº¾N THá»¨C Tá»ª TÃ€I LIá»†U:
{context_str}

YÃŠU Cáº¦U: {question}

CÃC CÃ‚U Há»ŽI:"""
            
            # Generate questions with fallback
            answer, used_model = self._generate_with_fallback(
                provider_name=provider_name,
                model_identifier=model_object,
                prompt=user_prompt,
                system_instruction=system_instruction,
                temperature=temperature or 0.9,
                max_tokens=2048
            )
            
            return {
                "answer": answer,
                "contexts": contexts,
                "model": used_model,
                "tokens_used": self._estimate_tokens(context_str + question, answer)
            }
        
        except Exception as e:
            print(f"âŒ Question generation error: {e}")
            raise Exception(f"Question generation failed: {e}")
    
    async def handle_homework(
        self,
        question: str,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Homework solver with step-by-step explanation
        Uses: Gemini Pro (best reasoning in FREE tier)
        """
        try:
            # Use Pro for complex reasoning
            provider_name, model_object = self.model_manager.get_model(
                task_type="homework_solver",
                complexity="high"
            )
            
            # System instruction
            system_instruction = """Báº¡n lÃ  gia sÆ° chuyÃªn nghiá»‡p, giÃºp há»c sinh hiá»ƒu sÃ¢u kiáº¿n thá»©c.

NHIá»†M Vá»¤:
HÆ°á»›ng dáº«n há»c sinh giáº£i bÃ i táº­p báº±ng cÃ¡ch:

1. **PHÃ‚N TÃCH Äá»€ BÃ€I**
   - XÃ¡c Ä‘á»‹nh yÃªu cáº§u
   - Dá»¯ liá»‡u Ä‘Ã£ cho
   - Äiá»u cáº§n tÃ¬m

2. **CÃCH TIáº¾P Cáº¬N**
   - PhÆ°Æ¡ng phÃ¡p giáº£i
   - CÃ¡c bÆ°á»›c cáº§n thá»±c hiá»‡n
   - LÃ½ do chá»n cÃ¡ch nÃ y

3. **Lá»œI GIáº¢I CHI TIáº¾T**
   - Tá»«ng bÆ°á»›c cá»¥ thá»ƒ
   - Giáº£i thÃ­ch logic
   - TÃ­nh toÃ¡n (náº¿u cÃ³)

4. **Káº¾T LUáº¬N**
   - ÄÃ¡p Ã¡n
   - Kiá»ƒm tra láº¡i
   - BÃ i há»c rÃºt ra

YÃŠU Cáº¦U:
- Giáº£i thÃ­ch dá»… hiá»ƒu
- KhÃ´ng bá» qua bÆ°á»›c nÃ o
- Khuyáº¿n khÃ­ch tÆ° duy
- Báº±ng tiáº¿ng Viá»‡t"""
            
            # Build user prompt
            user_prompt = f"""BÃ€I Táº¬P:
{question}

HÆ¯á»šNG DáºªN:"""
            
            # Generate homework solution with fallback
            answer, used_model = self._generate_with_fallback(
                provider_name=provider_name,
                model_identifier=model_object,
                prompt=user_prompt,
                system_instruction=system_instruction,
                temperature=temperature or 0.7,
                max_tokens=max_tokens or 3072
            )
            
            return {
                "answer": answer,
                "contexts": [],
                "model": used_model,
                "tokens_used": self._estimate_tokens(question, answer)
            }
        
        except Exception as e:
            print(f"âŒ Homework solver error: {e}")
            raise Exception(f"Homework solving failed: {e}")
    
    # ============================================
    # Helper Methods
    # ============================================
    def _generate_with_fallback(
        self,
        provider_name: str,
        model_identifier: str,
        prompt: str,
        system_instruction: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2048
    ) -> tuple[str, str]:
        """
        Generate text with automatic fallback to Groq on 429 errors.
        Returns: (answer, used_model_name)
        """
        try:
            answer = self.model_manager.generate_text(
                provider_name=provider_name,
                model_identifier=model_identifier,
                prompt=prompt,
                system_instruction=system_instruction,
                temperature=temperature,
                max_tokens=max_tokens
            )
            return answer, provider_name
        
        except Exception as gen_error:
            error_str = str(gen_error)
            
            # If rate limit (429), fallback to Groq
            if "429" in error_str or "Too Many Requests" in error_str:
                print(f"âš ï¸ {provider_name} rate limit reached, falling back to Groq...")
                
                # Get Groq model
                fallback_provider, fallback_model = self.model_manager.get_model(
                    task_type="general",
                    complexity="medium",
                    force_provider="groq"
                )
                
                # Retry with Groq
                answer = self.model_manager.generate_text(
                    provider_name=fallback_provider,
                    model_identifier=fallback_model,
                    prompt=prompt,
                    system_instruction=system_instruction,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                used_model = f"{fallback_provider} (fallback from {provider_name})"
                print(f"âœ… Fallback successful with {fallback_provider}")
                return answer, used_model
            else:
                # Other errors, re-raise
                raise
    
    async def _retrieve_contexts(
        self,
        query: str,
        user_id: str,
        document_ids: Optional[List[str]],
        top_k: int,
        score_threshold: float
    ) -> List[Dict[str, Any]]:
        """Retrieve contexts from Qdrant"""
        try:
            # Generate query embedding
            query_vector = embedding_service.embed_query(query)
            
            # Build filter
            filter_conditions = [
                FieldCondition(key="user_id", match=MatchValue(value=user_id))
            ]
            
            if document_ids:
                filter_conditions.append(
                    FieldCondition(key="document_id", match=MatchAny(any=document_ids))
                )
            
            query_filter = Filter(must=filter_conditions)
            
            # Search
            search_results = qdrant_manager.client.search(
                collection_name=qdrant_manager.collection_name,
                query_vector=query_vector,
                query_filter=query_filter,
                limit=top_k,
                score_threshold=score_threshold
            )
            
            # Fallback with lower threshold
            if len(search_results) < max(2, top_k // 2) and settings.RAG_ENABLE_FALLBACK:
                min_threshold = settings.RAG_MIN_SCORE_THRESHOLD
                if score_threshold > min_threshold:
                    print(f"âš ï¸ Fallback: Lowering threshold {score_threshold} -> {min_threshold}")
                    search_results = qdrant_manager.client.search(
                        collection_name=qdrant_manager.collection_name,
                        query_vector=query_vector,
                        query_filter=query_filter,
                        limit=top_k,
                        score_threshold=min_threshold
                    )
            
            # Format results
            contexts = []
            for result in search_results:
                contexts.append({
                    "chunk_id": result.id,
                    "score": result.score,
                    "chunk_text": result.payload.get("chunk_text", ""),
                    "chunk_index": result.payload.get("chunk_index", 0),
                    "document_id": result.payload.get("document_id", ""),
                    "file_name": result.payload.get("file_name", ""),
                    "title": result.payload.get("title", "")
                })
            
            return contexts
        
        except Exception as e:
            print(f"âŒ Context retrieval error: {e}")
            return []
    
    async def _retrieve_full_document(
        self,
        user_id: str,
        document_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """Retrieve all chunks from document(s) for summarization"""
        try:
            filter_conditions = [
                FieldCondition(key="user_id", match=MatchValue(value=user_id)),
                FieldCondition(key="document_id", match=MatchAny(any=document_ids))
            ]
            
            query_filter = Filter(must=filter_conditions)
            
            # Scroll to get all chunks (not search)
            results, _ = qdrant_manager.client.scroll(
                collection_name=qdrant_manager.collection_name,
                scroll_filter=query_filter,
                limit=100,  # Max chunks per document
                with_payload=True,
                with_vectors=False
            )
            
            # Sort by chunk_index
            contexts = []
            for result in results:
                contexts.append({
                    "chunk_id": result.id,
                    "score": 1.0,  # No score for scroll
                    "chunk_text": result.payload.get("chunk_text", ""),
                    "chunk_index": result.payload.get("chunk_index", 0),
                    "document_id": result.payload.get("document_id", ""),
                    "file_name": result.payload.get("file_name", ""),
                    "title": result.payload.get("title", "")
                })
            
            # Sort by chunk index
            contexts.sort(key=lambda x: x["chunk_index"])
            
            return contexts
        
        except Exception as e:
            print(f"âŒ Full document retrieval error: {e}")
            return []
    
    async def _fallback_chat(
        self,
        question: str,
        temperature: Optional[float],
        max_tokens: Optional[int]
    ) -> Dict[str, Any]:
        """Fallback to any available model"""
        try:
            # Try to get any model (will use fallback chain automatically)
            provider_name,model_object = self.model_manager.get_model(
                task_type="direct_chat",
                complexity="low"
            )
            
            # System instruction
            system_instruction = "Báº¡n lÃ  trá»£ lÃ½ thÃ´ng minh. Tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng."
            
            # Generate answer
            answer = self.model_manager.generate_text(
                provider_name=provider_name,
                model_identifier=model_object,
                prompt=question,
                system_instruction=system_instruction,
                temperature=temperature or 0.7,
                max_tokens=max_tokens or 2048
            )
            
            return {
                "answer": answer,
                "contexts": [],
                "model": f"{provider_name}-fallback",
                "tokens_used": self._estimate_tokens(question, answer)
            }
        
        except Exception as e:
            raise Exception(f"All models failed: {e}")
    

    
    def _estimate_tokens(self, input_text: str, output_text: str) -> int:
        """Rough token estimation"""
        return len(input_text.split()) + len(output_text.split())


# Global singleton instance
orchestrator = AIOrchestrator()
